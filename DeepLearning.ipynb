{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJOW1zCuothTR7akwH9ZP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18521039/covid19_fakenew-dectected/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GptPoZ8qKaFu"
      },
      "source": [
        "cd drive/My Drive/ĐỒ ÁN ML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvL6FmcDKm-4"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "dataset = pd.read_excel('Constraint_English_Train.xlsx')\r\n",
        "dataset.loc[:,'label'] = dataset.label.map({'real':0, 'fake':1})\r\n",
        "Y_train = dataset.iloc[:,-1]\r\n",
        "\r\n",
        "datatest = pd.read_excel('Constraint_English_Val.xlsx')\r\n",
        "datatest.loc[:,'label'] = datatest.label.map({'real':0, 'fake':1})\r\n",
        "Y_Val = datatest.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyETuXwJKpLp"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVDwpjQMKrim"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_puUFvCEKuDz"
      },
      "source": [
        "import re\r\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "\r\n",
        "corpus_train = []\r\n",
        "for tweet in dataset.values[:,1]:\r\n",
        "  tweet = re.sub('http\\S+', '', tweet) \r\n",
        "  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\r\n",
        "  tweet = strip_non_alphanum(tweet).lower().strip() \r\n",
        "  tweet = tweet.split() \r\n",
        "  ps = PorterStemmer()\r\n",
        "  tweet = [ps.stem(word) for word in tweet if word not in stopwords.words(\"english\")]\r\n",
        "  tweet = \" \".join(tweet)\r\n",
        "  corpus_train.append(tweet)\r\n",
        "\r\n",
        "corpus_test = []\r\n",
        "for tweet in datatest.values[:,1]:\r\n",
        "  tweet = re.sub('http\\S+', '', tweet) \r\n",
        "  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\r\n",
        "  tweet = strip_non_alphanum(tweet).lower().strip() \r\n",
        "  tweet = tweet.split() \r\n",
        "  ps = PorterStemmer()\r\n",
        "  tweet = [ps.stem(word) for word in tweet if word not in stopwords.words(\"english\")]\r\n",
        "  tweet = \" \".join(tweet)\r\n",
        "  corpus_test.append(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf3EwDIkK2Ni"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(list(corpus_train))\r\n",
        "\r\n",
        "x_tr_seq  = tokenizer.texts_to_sequences(corpus_train) \r\n",
        "x_val_seq = tokenizer.texts_to_sequences(corpus_test)\r\n",
        "\r\n",
        "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=20)\r\n",
        "x_val_seq = pad_sequences(x_val_seq, maxlen=20)\r\n",
        "print(x_tr_seq)\r\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1\r\n",
        "print(size_of_vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4znbKGzWK5O7"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def precision_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    return precision\r\n",
        "\r\n",
        "def f1_m(y_true, y_pred):\r\n",
        "    precision = precision_m(y_true, y_pred)\r\n",
        "    recall = recall_m(y_true, y_pred)\r\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMZYmB7jK_mI"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import *\r\n",
        "from keras.layers import *\r\n",
        "from keras.callbacks import *\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(size_of_vocabulary,300,input_length=20,trainable=True)) \r\n",
        "\r\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\r\n",
        "\r\n",
        "model.add(GlobalMaxPooling1D())\r\n",
        "\r\n",
        "model.add(Dense(64,activation='relu')) \r\n",
        "model.add(Dense(1,activation='sigmoid')) \r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc',f1_m, keras.metrics.Recall(), keras.metrics.Precision()]) \r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \r\n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "history = model.fit(np.array(x_tr_seq),np.array(Y_train),batch_size=128,epochs=10,validation_data=(np.array(x_val_seq),np.array(Y_Val)),verbose=1,callbacks=[mc,es])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}